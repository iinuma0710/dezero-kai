# DeZero-Kai の実装 その7

## Step 28. 関数の最適化
最適化とは、ある関数の最大値、または最小値を取る引数の値を求めることです。
ここでは、ローゼンブロック関数 $y = 100(x_1 - x_0^2)^2 + (x_0 - 1)^2$ を題材として、その最小値を取るときの $(x_0, x_1)$ を求めることを目指します。  
ローゼンブロック関数とその微分は以下のように求められます。

```python
def rosenbrock(x0, x1):
    y = 100 * (x1 - x0 ** 2) ** 2 + (x0 - 1) ** 2
    return y

x0 = Variable(np.array(0.0))
x1 = Variable(np.array(2.0))
y = rosenbrock(x0, x1)
y.backward()
print(x0.grad, x1.grad) # -2.0 400.0
```

一般に、ある地点における勾配は、必ずしも最大値や最小値を取る地点に向いていませんが、局所的に見れば勾配は関数の値を最も大きくさせる方向を示します。
そこで、ある地点から勾配方向に少し進み、進んだ先で勾配を再計算するという作業を繰り返すことで、最大値や最小値の場所に近づけると考えられます。
これを勾配降下法と呼び、以下のような繰り返し処理で実装できます。

```python
lr = 0.001      # 学習率
iters = 1000    # 繰り返し回数

for i in range(iters):
    print(x0, x1)

    y = rosenbrock(x0, x1)

    # 勾配の計算
    x0.cleargrad()
    x1.cleargrad()
    y.backward()

    # x0, x1 の更新
    x0.data -= lr * x0.grad
    x1.data -= lr * x1.grad
```

```lr``` は学習率 (learning rate) で、1回の更新幅を表します。
1000回の繰り返し操作では $(x0, x1)=(0.68349178, 0.4656506)$ となりますが、10000回とすると $(x0, x1)=(0.99449622, 0.98900063)$ となり、最適な $(x0, x1)=(1.0, 1.0)$ に近づいていきます。

## Step 29. ニュートン法による最適化
Step 28 では、勾配降下法で最適化をしてみましたが、探索が収束するのに数万回の更新が必要となります。
そこで、ニュートン法による最適化で更新回数を減らすことを考えます。
ニュートン法は、テイラー展開の式を2階微分までで打ち切った2次近似式を用います。

$$
f(x) \simeq f(a) + f^{\prime}(a)(x-a) + \frac{1}{2}f^{\prime\prime}(a)(x-a)^2
$$

2次関数の微分が0になる点は解析的に $x = a - \frac{f^{\prime}(a)}{f^{\prime\prime}(a)}$ と求められます。
ニュートン法では、この式に従って $x$ の値を更新します。  
例えば、$y = f(x)= x^4 - 2x^2$ の最適化を考えたとき、$f^{\prime}(x)=4x^3 - 4x$、$f^{\prime\prime}(x)=12x^2 - 4$ なので、以下のコードで最適化できます。

```python
def f(x):
    y = x ** 4 - 2 * x ** 2
    return y

def gx2(x):
    return 12 * x ** 2 -4

x = Variable(np.array(2.0))

iters = 10  # 繰り返し回数

for i in range(iters):
    print(i, x)

    y = f(x)

    # 勾配の計算
    x.cleargrad()
    y.backward()

    # x の更新
    x.data -= x.grad / gx2(x.data)
```

この場合、7回の更新で最小値にたどり着いていることがわかります。

## Step 30-32. 高階微分
これまでの、実装では2階以上の微分の計算を想定していませんでした。
具体的には、逆伝播で計算される微分値 ```grad``` が ```ndarray``` のインスタンスを参照して計算されており、その際に新たな計算グラフを構築していませんでした。
そこで、```grad``` を ```Variable``` のインスタンスとして実装することとし、1階微分の計算時にも計算グラフを構築するように実装を変更します。
これによって、1階微分の逆伝播として2階微分、2階微分の逆伝播として3階微分といったように、高階微分を簡単に計算できるようになります。  
まず、微分のスタート地点として、```Variable``` クラスの以下の個所を変更します

```python
class Variable():
   ...

    def backward(self, retain_grad=False):
        if self.grad is None:
            self.grad = Variable(np.ones_like(self.data))   # 変更後
            # self.grad = np.ones_like(self.data)           # 変更前
        ...
```

次に、```core.py``` で定義した各関数クラスの ```backward``` メソッドを変更します。
例えば ```Mul``` クラスでは、```self.inputs``` の各要素から ```data``` 属性 (```ndarray``` インスタンス) を取り出していますが、```Variable``` のまま計算するように変更します。

```python
class Mul(Function):
    ...
    
    def backward(self, gy):
        x0, x1 = self.inputs    # 変更後
        # x0, x1 = self.inputs[0].data, self.inputs[1].data # 変更前
        return gy * x1, gy * x0
```

以上で、高階微分を簡単に計算できるようになりました。
しかし、高階微分が必要となる場面は必ずしも多くありません。
高階微分を計算しないのに、新しい計算グラフを作るのはメモリの無駄遣いです。
そこで、```create_graph``` 引数で高階微分が必要な場合にのみ逆伝播時に計算グラフを構築するように変更します。

```python
class Variable():
   ...

    def backward(self, retain_grad=False, create_graph=False):
        ...

        while funcs:
            f = funcs.pop()
            gys = [output().grad for output in f.outputs]

            with using_config('enable_backprop', create_graph):
                gxs = f.backward(*gys)
                if not isinstance(gxs, tuple):
                    gxs = (gxs, )

                for x, gx in zip(f.inputs, gxs):
                    if x.grad is None:
                        x.grad = gx
                    else:
                        x.grad = x.grad + gx

                    if x.creator is not None:
                        add_func(x.creator)
            ...
```

## Step 33. ニュートン法を使った最適化
Step 32 までの実装変更で、2階微分を自動的に計算できるようになります。

```python
def f(x):
    y = x ** 4 - 2 * x ** 2
    return y


x = Variable(np.array(2.0))
y = f(x)

# 1階微分
y.backward(create_graph=True)
print(x.grad)   # variable(24.0)

# 2階微分
gx = x.grad
x.cleargrad()
gx.backward()
print(x.grad)   # variable(44.0)
```

これをもとに、ニュートン法による最適化を試してみます。

```python
def f(x):
    y = x ** 4 - 2 * x ** 2
    return y

x = Variable(np.array(2.0))
iters = 10

for i in range(10):
    print(i, x)

    y = f(x)

    x.cleargrad()
    y.backward(create_graph=True)

    gx = x.grad
    x.cleargrad()
    gx.backward()
    gx2 = x.grad

    x.data -= gx.data / gx2.data
```

先ほどと同じ結果が得られました。