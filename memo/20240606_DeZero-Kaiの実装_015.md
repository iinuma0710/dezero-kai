# DeZero-Kai の実装 その13

## Step 46. Optimizer によるパラメータの更新
これまで、モデルの学習には最適化手法として勾配降下法を用いてきましたが、その他にも様々な最適化手法が提案されています。
このステップではまず、これらの実装の基盤となる ```Optimizer``` クラスを実装します。
```Optimizer``` クラスでは、```setup``` メソッドで最適化の対象となるモデルを設定し、```update``` メソッドでパラメータの更新を行います。
ただし、アップデート方法の詳細な実装は ```update_one``` で行うこととし、```Optimizer``` を継承した先のクラスで実装します。
また、最適化において前処理が必要な場合には、```add_hook``` メソッドを通して、前処理を行う関数を追加します。

```dzrkai/optimizers.py```
```python
class Optimizer:
    def __init__(self):
        self.target = None
        self.hooks = []

    def setup(self, target):
        self.target = target
        return self
    
    def update(self):
        # None 以外のパラメータを抽出
        params = [p for p in self.target.params() if p.grad is not None]

        # 前処理
        for f in self.hooks:
            f(params)

        # パラメータの更新
        for param in params:
            self.update_one(param)

    def update_one(self, param):
        raise NotImplementedError()
    
    def add_hooks(self, f):
        self.hooks.append(f)
```

それでは、勾配降下法によってパラメータを更新する ```SGD``` クラスを実装してみます。
SGD は Stochastic Gradient Descent の略で、日本語では確率的勾配降下法と呼ばれます。
意味的には、一部のデータのみをランダムに選び出して勾配降下法を行うのが SGD であり、全データを用いる勾配降下法とは厳密には異なりますが、処理自体は同じになります。

```dzrkai/optimizers.py```
```python
class SGD(Optimizer):
    def __init__(self, lr=0.01):
        super().__init__()
        self.lr = 0.01

    def update_one(self, param):
        param.data -= self.lr * param.grad.data
```

SGD を用いて前のステップと同じ問題を解いてみます。

```python
# データの生成
np.random.seed(0)
x = np.random.rand(100, 1)
y = np.sin(2 * np.pi * x) +np.random.rand(100, 1)

# ハイパーパラメータの設定
lr = 0.2
iters = 10000
hidden_size = 10
    
# モデルと最適化手法のインスタンス作成
model = MLP((hidden_size, 1))
optimizer = optimizers.SGD(lr)
optimizer.setup(model)

# 学習の開始
for i in range(iters):
    y_pred = model(x)
    loss = F.mean_squared_error(y, y_pred)

    model.cleargrads()
    loss.backward()
    optimizer.update()

    if i % 1000 == 0:
        print("(iter: ", i, ") loss: ", loss)
```

これまで、すべての問題を勾配降下法で解いてきましたが、他にも Adam や AdaDelta、AdaGrad など、様々な最適化手法が提案されています。
ここでは、Momentum と呼ばれる手法を ```MomentumSGD``` クラスで実装してみましょう。
Momentum では、物理における「速度」の概念を導入し、勾配方向への移動幅の調整を慣性的に行う手法です。
定数 (momentum) $\alpha$ と学習率 $\eta$ を用いて、以下の式でパラメータを更新します。

$$
\bold v \leftarrow \alpha \bold v - \eta \frac{\partial L}{\partial \bold W} \\
\bold W \leftarrow \bold W + \bold v
$$

実装は以下の通りです。

```dzrkai/optimizers.py```
```python
class MomentumSGD(Optimizer):
    def __init__(self, lr=0.01, momentum=0.9):
        super().__init__()
        self.lr = lr
        self.momentum = momentum
        self.vs = {}

    def update_one(self, param):
        v_key = id(param)
        if v_key not in self.vs:
            self.vs[v_key] = np.zeros_like(param.data)

        v = self.vs[v_key]
        v *= self.momentum
        v -= self.lr * param.grad.data
        param.data += v
```

先ほどの回帰問題において、```SGD``` を ```MomentumSGD``` に置き換えるだけで使えるようになります。