# DeZero-Kai の実装 その16

## Step 47. ソフトマックス関数と交差エントロピー誤差
このステップでは、分類問題を解くために必要なソフトマックス関数と交差エントロピー誤差を実装します。
それに先立って、まずは、あるテンソルから指定された要素を取り出す ```get_item``` 関数を定義します。

```dzrkai/functions.py```
```python
class GetItem(Function):
    def __init__(self, slices):
        self.slices = slices

    def forward(self, x):
        y = x[self.slices]
        return y
    
    def backward(self, gy):
        x, = self.inputs
        f = GetItemGrad(self.slices, x.shape)
        return f(gy)
    
class GetItemGrad(Function):
    def __init__(self, slices, in_shape):
        self.slices = slices
        self.in_shape = in_shape

    def forward(self, gy):
        gx = np.zeros(self.in_shape)
        np.add.at(gx, self.slices, gy)
        return gx
    
    def backward(self, ggx):
        return get_item(ggx, self.slices)
    
def get_item(x, slices):
    return GetItem(slices)(x)
```

```get_item``` の勾配は、取り出された要素の箇所だけを伝播させます。
入力の形状に合わせて作成した全要素がゼロで初期化された行列に、```np.add.at``` 関数で勾配を指定のインデックスに足し合わせていくことでこれを実現しています。
この ```get_item``` 関数を、```Variable``` の ```__getitem__``` メソッドに設定しておくと、変数のスライスを呼び出すときに ```get_item``` が使われるようになります。

```dzrkai/core.py```
```python
def setup_variable():
    ...
    Variable.__getitem__ = dzrkai.functions.get_item
```

次に、ソフトマックス (Softmax) 関数を定義します。
ニューラルネットワークが出力するのはただの数値ですが、これを確率に変換するのがソフトマックス関数です。
ネットワークの出力を$[y_1, y_2, \cdots, y_n]$ とすると、$y_k$ に対応するソフトマックス関数の出力 $p_k$ は。以下の式で定義されます。

$$
p_k = \frac{exp(y_k)}{\Sigma_{i=1}^n exp(y_i)}
$$

ソフトマックス関数を通すことで、$p_1+p_2+\cdots+p_n=1$ となるので、出力結果を確率のように扱えるようになります。
ソフトマックス関数のある出力 $p_i$ を、ある変数 $y_k$ について微分すると以下のようになります。

$$
\frac{\partial p_i}{\partial y_k}=
\begin{cases}
    p_k(1-p_k) & \text{if i=k} \\
    -p_i p_k & \text{otherwise}
\end{cases}
$$

これを踏まえて、ソフトマックス関数の演算を行うクラス ```Softmax``` は、以下の通り定義できます。

```dzrkai/functions.py```
```python
class Softmax(Function):
    def __init__(self, axis=1):
        self.axis = axis

    def forward(self, x):
        y = x - x.max(axis=self.axis, keepdims=True)
        y = np.exp(y)
        y /= y.sum(axis=self.axis, keepdims=True)
        return y
    
    def backward(self, gy):
        y = self.outputs[0]()
        gx = y * gy
        sumdx =gx.sum(axis=self.axis, keepdims=True)
        gx -= y * sumdx
        return gx
    
def softmax(x, axis=1):
    return Softmax(axis)(x)
```

```Softmax``` クラスの ```forward``` メソッドでは、オーバーフロー対策として入力の各要素から入力の最大値を引いています。  
続いて、交差エントロピー誤差 (cross entropy error) を実装します。
交差エントロピー誤差は多値類で用いられる誤差であり、以下の式で定義されます。

$$
L=-\Sigma_k t_k \log p_k
$$

この式において、$p_k$ はソフトマックス関数を通したニューラルネットワークの出力 $\bold p = (p_1, p_2, \cdots, p_n)$、$t_k$ は 教師データのそれぞれ $k$ 番目の要素を示しています。
教師データはワンホット (one-hot) ベクトルで与えられているものとし、正解のクラスが $1$、それ以外が $0$ となります。
ゆえに、その微分は以下の式で与えられます。

$$
\frac{\partial L}{\partial x_k}=
\begin{cases}
    -\frac{1}{p_k} & \text{if correct } \\
    0 & \text{otherwise}
\end{cases}
$$

通常、ソフトマックス関数と交差エントロピー誤差はセットで使われるので、これらを合わせた ```SoftmaxCrossEntropy``` クラスを定義しておきます。
そのまま、素直に実装しても良いのですが、$K$ 番目が正解データであるとして次のように式を変形します。

$$
\begin{split}
CrossEntropyError \bigl(Softmax(\bold x) \bigr) &= - \sum_k t_k \log \biggl( \frac{\exp \bigl(x_k - max(\bold x) \bigr)}{\sum_l \exp \bigl(x_l - max(\bold x) \bigr)} \biggr) \\
&= - \Bigl(\bigl(x_K - max(\bold x) \bigr) - \log \sum_l \exp \bigl(x_l - max(\bold x) \bigr) \Bigr)
\end{split}
$$

また、上式の計算結果を $L$ として、ある $x_k$ について微分すると、以下の式のようになります。

$$
\frac{\partial L}{\partial x_k} = 
\frac{\partial L}{\partial x_k}=
\begin{cases}
    Softmax(x) - 1 & \text{if k=K } \\
    Softmax(x) & \text{otherwise}
\end{cases}
$$

ここでは、$\log \sum_l \exp \bigl(x_l - max(\bold x) bigr)$ の $x_k$ についての微分が、$\frac{\exp \bigl(x_k - max(\bold x) \bigr)}{\sum_l \exp \bigl(x_l - max(\bold x) \bigr)} = Softmax(x_k)$ であることを利用しています。
上記の式は1次元の場合ですが、これを多次元に拡張して以下のように実装できます。


```dzrkai/functions.py```
```python
class SoftmaxCrossEntropy(Function):
    def forward(self, x, t):
        N = x.shape[0]
        log_z = utils.logsumexp(x, axis=1)
        log_p = x - log_z
        log_p = log_p[np.arange(N), t.ravel()]
        y = -log_p.sum() / np.float32(N)
        return y
    
    def backward(self, gy):
        x, t = self.inputs
        N, CLS_NUM = x.shape

        gy *= 1 / N
        y = softmax(x)
        t_onehot = np.eye(CLS_NUM, dtype=t.dtype)[t.data]

        y = (y - t_onehot) * gy
        return y

def softmax_cross_entropy(x, t):
    return SoftmaxCrossEntropy()(x, t)
```

また、上記で ```logsumexp``` 関数は以下のように定義されます。

```dzrkai/utils.py```
```python
def logsumexp(x, axis=1):
    m = x.max(axis=axis, keepdims=True)
    y = x - m
    np.exp(y, out=y)
    s = y.sum(axis=axis, keepdims=True)
    np.log(s, out=s)
    m += s
    return m
```