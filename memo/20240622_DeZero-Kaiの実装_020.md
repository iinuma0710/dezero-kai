# DeZero-Kai の実装 その20

## Step 52. GPU 対応
ディープラーニングで行われる計算の多くを行列積が占めています。
このステップでは、行列積を効率的に処理するために、CuPy を用いて GPU 処理に対応します。
[CuPy](https://cupy.dev/) は GPU で NumPy のような行列演算を実行できるライブラリです。
NumPy と CuPy に完全な互換性はありませんが、多くの場合でコードの ```numpy``` や ```np``` を ```cupy``` や ```cp``` に置き換えるだけで、簡単に GPU を使えるようになります。  
まず、NumPy 配列から CuPy 配列に変換するには、CuPy の ```asarray``` 関数を用います。

```python
cupy_array = cupy.asarray(numpy_array)
```

逆に、CuPy 配列から NumPy 配列の変換は、CuPy の ```asnumpy``` 関数を使います。

```python
numpy_array = cupy.asnumpy(cupy_array)
```

また、```cupy.get_array_module``` 関数を使って、ある配列が NumPy の配列なのか、CuPy の配列なのかを判定することができます。
次に示す通り、引数で渡された変数が NumPy 配列の場合は ```numpy``` の、CuPy 配列の場合は ```cupy``` のモジュールを返します。

```python
x = np.array([1, 2, 3])
xp = cp.get_array_module(x) # xp == np

x = cp.array([1, 2, 3])
xp = cp.get_array_module(x) # xp == cp
```

この仕組みを使って NumPy でも CuPy でも動作するコードを書くことができます。
ただ、上記の関数はいずれも NumPy や CuPy の配列を引数に取るので、DeZero-Kai の ```Variable``` を引数に取れるように、改めてラッパ関数を定義します。
GPU 周りの操作に必要なコードは ```dzrkai/cuda.py``` にまとめて記述しておきます。

```python
import numpy as np

# CuPy がインストールされていない場合のエラー回避
gpu_enable = True
try:
    import cupy as cp
    cupy = cp
except ImportError:
    gpu_enable = False

from dzrkai import Variable


def get_array_module(x):
    if isinstance(x, Variable):
        x = x.data

    # GPU が無効の場合は常に numpy のモジュールを返す
    if not gpu_enable:
        return np
    
    xp = cp.get_array_module(x)
    return xp


def as_numpy(x):
    if isinstance(x, Variable):
        x = x.data

    if np.isscalar(x):
        return np.array(x)
    elif isinstance(x, np.ndarray):
        return x
    
    return cp.asnumpy(x)


def as_cupy(x):
    if isinstance(x, Variable):
        x = x.data

    if not gpu_enable:
        raise Exception('CuPy cannot be loaded. Install CuPy!')
    
    return cp.asarray(x)
```

これらの関数を用いて、これまでの実装に GPU 対応コードを追加していきます。
まず、```Variable``` クラスに以下のコードを追加します。

```dzrkai/core.py```
```python
...
# CuPy のインポート
try:
    import cupy
    array_types = (np.ndarray, cupy.ndarray)
except ImportError:
    array_types = (np.ndarray)
...

class Variable():
    __array_priority__ = 200

    def __init__(self, data, name=None):
        if data is not None:
            # data の型チェックで CuPy の配列も許容
            if not isinstance(data, array_types):
                raise TypeError("{} is not supported".format(type(data)))

    ...

    def backward(self, retain_grad=False, create_graph=False):
        if self.grad is None:
            # NumPy と CuPy のどちらの配列かによってモジュールを切り替える
            xp = dzrkai.cuda.get_array_module(self.data)
            self.grad = Variable(xp.ones_like(self.data))

    ...

    # CuPy 配列を NuuPy 配列に変換 (GPU => CPU にデータを移動)
    def to_cpu(self):
        if self.data is not None:
            self.data = dzrkai.cuda.as_numpy(self.data)
    
    # NumPy 配列を CuPy 配列に変換 (CPU => GPU にデータを移動)
    def to_gpu(self):
        if self.data is not None:
            self.data = dzrkai.cuda.as_cupy(self.data)
```

コードの内容としては、コメントの通り NumPy と CuPy の両方の配列を保持できるようにコードを変更し、デバイス間でデータをやり取るするメソッドを追加しています。  
次に、```Layer``` クラスについて、パラメータを CPU と GPU でやり取りするメソッドを追加します。

```dzrkai/layers.py```
```python
class Layer:
    ...

    def to_cpu(self):
        for param in self.params():
            param.to_cpu()
            
    def to_gpu(self):
        for param in self.params():
            param.to_gpu()
```

また、```DataLoader``` クラスにも、入力データを GPU で扱えるよう以下の通り実装を変更・追加します。

```dzrkai/dataloaders.py```
```python
class DataLoader:
    def __init__(self, dataset, batch_size, shuffle=True, gpu=False):
        ...
        self.gpu = gpu
        self.reset()

    ...
    
    def __next__(self):
        ...
        xp = cuda.cupy if self.gpu else np
        x = xp.array([example[0] for example in batch])
        t = xp.array([example[1] for example in batch])

        self.iteration += 1
        return x, t

    def to_cpu(self):
        self.gpu = False

    def to_gpu(self):
        self.gpu = True
```

最後に、関数や四則演算で NumPy の関数を使っている箇所を、データの型によってモジュールを使い分けるように変更します。
例えば、```Sin``` 関数クラスでは、```get_array_module``` を使って次のように書き換えます。

```dzrkai/functions.py```
```python
class Sin(Function):
    def forward(self, x):
        xp = cuda.get_array_module(x)
        y = xp.sin(x)
        return y
    ...
```

また、四則演算については、例えば ```add``` 関数は以下のように変更します。

```dzrkai/core.py```
```python
def as_array(x, array_modele=np):
    if np.isscalar(x):
        return array_modele.array(x)
    return x

def add(x0, x1):
    x1 = as_array(x1, dzrkai.cuda.get_array_module(x0.data))
    return Add()(x0, x1)
```

それでは、GPU で MNIST を学習してみましょう。
学習コードは以下のようになります。

```python
# ハイパーパラメータの設定
max_epoch = 5
batch_size = 100
hidden_size = 1000

# データセットの読み込みとデータローダの準備
train_set = dzrkai.datasets.MNIST(train=True, transform=preprocess)
train_loader = DataLoader(train_set, batch_size, shuffle=True)

# モデルと最適化手法の準備
model = MLP((hidden_size, 10))
optimizer = optimizers.SGD().setup(model)

# GPU にモデルを送る
if dzrkai.cuda.gpu_enable:
    train_loader.to_gpu()
    model.to_gpu()

# 学習
for epoch in range(max_epoch):
    start = time.time()

    sum_loss = 0
    for x, t in train_loader:
        y = model(x)
        loss = F.softmax_cross_entropy(y, t)
        model.cleargrads()
        loss.backward()
        optimizer.update()
        sum_loss += float(loss.data) * len(t)

    # 学習経過の表示
    elapsed_time = time.time() - start
    avg_loss = sum_loss / len(train_set)
    print("epoch {}, loss: {:.4f}, time: {:.4f}[sec]".format(epoch + 1, avg_loss,  elapsed_time))
```

GPU で実行したときと CPU で実行したときの計算時間は以下の通りです。

```bash
# GPU 実行時
$ python step52.py
epoch 1, loss: 1.9211, time: 5.8377[sec]
epoch 2, loss: 1.2888, time: 3.1687[sec]
epoch 3, loss: 0.9260, time: 3.2503[sec]
epoch 4, loss: 0.7394, time: 3.2345[sec]
epoch 5, loss: 0.6341, time: 3.2698[sec]

# GPU 実行時
$ python step52.py
epoch 1, loss: 1.9260, time: 4.4783[sec]
epoch 2, loss: 1.2908, time: 3.9070[sec]
epoch 3, loss: 0.9254, time: 4.4403[sec]
epoch 4, loss: 0.7387, time: 3.8949[sec]
epoch 5, loss: 0.6334, time: 4.2610[sec]
```

GPU 実行時には、1エポック目は GPU へのデータ転送に時間がかかるため 5.84 秒ほどかかっていますが、2エポック目以降は 3.17 秒から 3.27 秒で、CPU よりも速く処理できています。
ただ、CPU の性能が高い (Ryzen 9 3950X) ためか、CPU における実行速度も悪くないので、期待したほどの高速化はできていませんでした。

※ (追記) GPU 実行時の処理時間はばらつきが大きく、最短で1エポック 2.30 秒で実行できることもありました。